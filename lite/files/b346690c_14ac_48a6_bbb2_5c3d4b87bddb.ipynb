{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python",
            "language": "python",
            "name": "python"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "id": "07be4d51",
            "cell_type": "markdown",
            "source": "<div class='alert alert-warning'>\n\nSciPy's interactive examples with Jupyterlite are experimental and may not always work as expected. Execution of cells containing imports may result in large downloads (up to 60MB of content for the first import from SciPy). Load times when importing from SciPy may take roughly 10-20 seconds. If you notice any problems, feel free to open an [issue](https://github.com/scipy/scipy/issues/new/choose).\n\n</div>",
            "metadata": {}
        },
        {
            "id": "ebc88d51",
            "cell_type": "markdown",
            "source": "We can use this function to calculate the binary logistic loss also\nknown as the binary cross entropy. This loss function is used for\nbinary classification problems and is defined as:\n\n$$ L = 1/n * \\sum_{i=0}^n -(y_i*log(y\\_pred_i) + (1-y_i)*log(1-y\\_pred_i)) $$\nWe can define the parameters `x` and `y` as y and y_pred respectively.\ny is the array of the actual labels which over here can be either 0 or 1.\ny_pred is the array of the predicted probabilities with respect to\nthe positive class (1).\n",
            "metadata": {}
        },
        {
            "id": "56175acf",
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "source": "import numpy as np\nfrom scipy.special import xlogy\ny = np.array([0, 1, 0, 1, 1, 0])\ny_pred = np.array([0.3, 0.8, 0.4, 0.7, 0.9, 0.2])\nn = len(y)\nloss = -(xlogy(y, y_pred) + xlogy(1 - y, 1 - y_pred)).sum()\nloss /= n\nloss",
            "outputs": [
                {
                    "output_type": "execute_result",
                    "metadata": {},
                    "data": {
                        "text/plain": "0.29597052165495025"
                    },
                    "execution_count": null
                }
            ]
        },
        {
            "id": "50d38374",
            "cell_type": "markdown",
            "source": "A lower loss is usually better as it indicates that the predictions are\nsimilar to the actual labels. In this example since our predicted\nprobabilities are close to the actual labels, we get an overall loss\nthat is reasonably low and appropriate.",
            "metadata": {}
        }
    ]
}