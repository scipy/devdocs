{
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python",
            "language": "python",
            "name": "python"
        },
        "language_info": {
            "name": "python"
        }
    },
    "cells": [
        {
            "id": "48d173b7",
            "cell_type": "markdown",
            "source": "<div class='alert alert-warning'>\n\nSciPy's interactive examples with Jupyterlite are experimental and may not always work as expected. Execution of cells containing imports may result in large downloads (up to 60MB of content for the first import from SciPy). Load times when importing from SciPy may take roughly 10-20 seconds. If you notice any problems, feel free to open an [issue](https://github.com/scipy/scipy/issues/new/choose).\n\n</div>",
            "metadata": {}
        },
        {
            "id": "e81d7ade",
            "cell_type": "markdown",
            "source": "This example demonstrates that ``isotonic_regression`` really solves a\nconstrained optimization problem.\n",
            "metadata": {}
        },
        {
            "id": "14d0595f",
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "source": "import numpy as np\nfrom scipy.optimize import isotonic_regression, minimize\ny = [1.5, 1.0, 4.0, 6.0, 5.7, 5.0, 7.8, 9.0, 7.5, 9.5, 9.0]\ndef objective(yhat, y):\n    return np.sum((yhat - y)**2)\ndef constraint(yhat, y):\n    # This is for a monotonically increasing regression.\n    return np.diff(yhat)\nresult = minimize(objective, x0=y, args=(y,),\n                  constraints=[{'type': 'ineq',\n                                'fun': lambda x: constraint(x, y)}])\nresult.x",
            "outputs": [
                {
                    "output_type": "execute_result",
                    "metadata": {},
                    "data": {
                        "text/plain": "array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\n       5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\n       9.25      ])"
                    },
                    "execution_count": null
                }
            ]
        },
        {
            "id": "890893d7",
            "cell_type": "code",
            "metadata": {},
            "execution_count": null,
            "source": "result = isotonic_regression(y)\nresult.x",
            "outputs": [
                {
                    "output_type": "execute_result",
                    "metadata": {},
                    "data": {
                        "text/plain": "array([1.25      , 1.25      , 4.        , 5.56666667, 5.56666667,\n       5.56666667, 7.8       , 8.25      , 8.25      , 9.25      ,\n       9.25      ])"
                    },
                    "execution_count": null
                }
            ]
        },
        {
            "id": "a14ddf40",
            "cell_type": "markdown",
            "source": "The big advantage of ``isotonic_regression`` compared to calling\n``minimize`` is that it is more user friendly, i.e. one does not need to\ndefine objective and constraint functions, and that it is orders of\nmagnitudes faster. On commodity hardware (in 2023), for normal distributed\ninput y of length 1000, the minimizer takes about 4 seconds, while\n``isotonic_regression`` takes about 200 microseconds.",
            "metadata": {}
        }
    ]
}